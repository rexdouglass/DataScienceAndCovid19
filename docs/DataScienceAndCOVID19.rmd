---
title: "The Data Science of COVID-19 Spread: Some Troubling Current and Future Trends"
date: "`r format(Sys.time(), '%d %B, %Y')`"
author:
  - name: Rex Douglass
    email: rexdouglass@gmail.com
    affiliation: University of California, San Diego
  - name: Thomas Leo Scherer
    email: tlscherer@ucsd.edu
    affiliation: University of California, San Diego
address:
  - code: University of California San Diego
    address: University of California, San Diego, La Jolla,CA, 92093
output: 
  html_notebook: default
  rticles::plos_article: default
bibliography: cpas_covid_19.bib

# Some paper adjustments
---

[2550/2500 words. Working draft. Comments welcome!]

<!-- 
abstract: Abstract here
author_summary: |
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur eget porta erat. Morbi consectetur est vel gravida pretium. Suspendisse ut dui eu ante cursus gravida non sed sem. Nullam sapien tellus, commodo id velit id, eleifend volutpat quam. Phasellus mauris velit, dapibus finibus elementum vel, pulvinar non tellus. Nunc pellentesque pretium diam, quis maximus dolor faucibus id. Nunc convallis sodales ante, ut ullamcorper est egestas vitae. Nam sit amet enim ultrices, ultrices elit pulvinar, volutpat risus.
-->

# Introduction

The SARS-COV-2 global pandemic has exposed weaknesses throughout our institutions, and the sciences are no exception. Given the deluge of official statistics and 300+ new COVID-19 working papers posted each day^[“COVID-19 Primer.” Accessed August 17, 2020. https://covid19primer.com/.], it is imperative for both consumers and producers of COVID-19 knowledge to be clear on what we do and do not know. In this brief review, we enumerate ways that data science has highlighted these weaknesses and is helping to address them.

<!-- There is a growing effort to forecast how the world will differ after the COVID-19 pandemic. Unfortunately, such en effort presupposes a clarity of measurement and understanding about the COVID-19 pandemic that largely does not exist now and might never materialize. This brief note outlines ways in which the COVID-19 pandemic caught our political and scientific institutions off guard and how the literally global effort to catch up will be able to close the distance. Where possible we highlight some places for optimism, some places for improvement, and some places to level expectations. -->

In terms of understanding where we are, how we got here, and what is likely to follow, here are some things we need to know. We want to know the rate of spread of COVID-19 in a population $R$, over time $R_{t}$, across different political and demographic communities $R_{ct}$, and prior to any non-pharmaceutical interventions $R_{c0}$. We want to know how many cases of active infection exist in a community $I_{ct}$ and how many of those infections resulted in death $D_{ct}$. We want to know the causal effect of interventions $X_{ct}$ on say rate of spread, between the observed treated populations $R_{ct1}$ and the counterfactual populations had they not been treated $R_{ct0}$. To do so, we need some plausible causal identification strategy that allows us to account for the fact that interventions are themselves chosen and implemented in response to changes in $R_{ct}$, and that many outside factors likely drive both $R_{ct}$ and $X_{ct}$ simultaneously. These unknowns give rise to fundamental problems of measurement, inference, and interpretation.

# Measurement

There is no direct measure of $I_{ct}$. The closest we have are serological estimates of Cumulative Infections $CI_{ct}$. These are surveys that measure the presence of antibodies in a given population which is an indication of what percent have had COVID-19 at some point in the past, and cannot tell us how recently. <!--[ANY EVIDENCE THAT ANTIBODY PRESENCE CAN FADE?]  A: Sorta. It fades in terms of immunity, but how low it can get before these tests can't detect it anymore I don't know. --> They are still rarely available and they have false positive rates that make them inappropriate for populations early in their spread [@peelingSerologyTestingCOVID192020]. Further, they require careful sampling designs comparable to opinion surveys, otherwise their findings may not be representative of the full underlying population.

More commonly available are Confirmed Cases $CC_{ct}$. COVID-19 tests are administered in a jurisdiction, and positive results are anonymized, tabulated, reported, and aggregated by increasingly nested bureaucracies. These bureaucracies are concerned primarily with releasing legally required contemporary measurements and not maintaining consistent historical time series. This has resulted in the world's largest, most desperate scavenger hunt to scrape, transcribe, and translate counts disseminated in oral briefings, public websites, PDFs, and even static images [@alamoCovid19OpenDataResources2020]. Teams from every country are working in often uncoordinated and duplicated efforts to compile government reporting into consistent panel data; these teams include newspapers [@albertsunNewYorkTimes2020], nonprofits [@usafactsUSCoronavirusCases2020], large private companies [@chiqunzhangBingCOVID19Data2020; @ashleywolfYahooKnowledgeGraph2020], consortiums of volunteers [@yangCovidNetBringData2020; @jzohrabCOVIDAtlasLi2020; @covid19indiaorg2020tracker], and Wikipedians.

The resulting ecosystem of panel datasets vary in spatial and temporal coverage, have little metadata about sources or changing definitions, and generally do not handle revisions to past counts from reporting sources. Direct comparisons between sources reveal worrying disagreements and temporal artifacts like reporting delays, seasonalities, discontinuities, and sudden revisions in counts both upwards and downwards [@wangComparingIntegratingUS2020]. It is not obvious how to correctly account for these problems or adjudicate between conflicting sources without a clear ground truth. There also is no permanent archive of the raw source material meaning reconstructing the full chain of evidence may no longer be possible. 

Likewise, we do not have direct measures of Deaths $D_{ct}$ but only Confirmed Deaths $CD_{ct}$. $CD_{ct}$ suffers from all of the problems of Confirmed Cases $CC_{ct}$ except for possibly less under-reporting depending on if the person died at home or in medical care. Choosing $CD_{ct}$ as the lesser of two evils, many projects attempt to take plausible values of the Infected Fatality Rate $IFR=D/I$ to back out an estimation for $I_{ct}$ [@meyerowitz-katzSystematicReviewMetaanalysis2020]. Others have turned to estimating Excess Deaths $ED_{ct}$, which is a number proportional to the number of total deaths reported in an area above what would be expected given the number of deaths reported in previous years [@weinbergerEstimationExcessDeaths2020]. $ED_{ct}$ is also not a direct estimate of $D_{ct}$ as it can include deaths that were not caused by COVID-19 directly, e.g. other health conditions that received inadequate care during this period, and similarly can undercount the number of COVID-19 caused deaths as lockdowns reduce mobility and economic activity that might typically lead to deaths, e.g. car accidents.

Confirmed case and death counts mechanically depend on testing, but records of tests administered $T_{ct}$ are even worse. In the U.S., much of what we know about trends in testing patterns come from journalistic efforts like the Covid-Tracking Project [@zachliptonCovidTrackingProject2020]. They encountered all of the regular problems plus additional ones specific to ambiguity to what kind of test count is being reported (testing encounters, number of people tested, number of swabs tested, etc). The type of test performed (and its false positive and false negative rate) is almost never included as metadata. Nor are the rules about how tests are being rationed and distributed being recorded systematically.

The general failure to track COVID-19 spread directly has led to a proliferation of innovative attempts to use other signals such as web searches, searches of medical databases, social media posts, fevers reported by home thermometer, and traditional flu symptom survailence networks [@koganEarlyWarningApproach2020]. While promising, proxy measures require ground truthing and regular calibration using something like regularly timed serological surveys on smaller geographic samples of the population. It is precisely the lack of such capabilities that are motivating the search for alternatives in the first place.

Finally, non-pharmaceutical interventions are tracked by several academic and nonprofit teams [@haleVariationGovernmentResponses2020; @chengCOVID19GovernmentResponse2020a]. These interventions are intended to limit human mobility which is more directly measured by cell phone data which are being provided by companies like Google, Apple, and SafeGraph. <!--need the cite -->

# Inference

The workhorse theoretical model for infectious disease spread is the Susceptible, Exposed, Infectious, and Removed (SEIR) compartmental model [@brauerMathematicalModelsPopulation2012]. The intuition behind the SEIR model is that there are mechanical relationships, such as previous infections or deaths removing candidates from infection, the timing between exposure to the next possible transmission, and the degree to which immunity may exist in the population, which induce nonlinearities in disease spread. Disease spreads slowly at first, accelerates, and then burns out if left to its own devices. SEIR should be considered the theoretical floor for analysis, and a entire menagerie of extensions account for demography, testing, mobility, social networks, etc. 

The necessity of directly including testing in models of disease spread can't be understated. Per capita cases are so temporarily correlated with per capita testing rates they are more of a proxy of testing availability than actual cases [@kaashoekCOVID19PositiveCases2020a]. Spatially, per capita testing rates correlate with urbanity and a wide range of co-morbids [@souchCommentaryRuralUrban2020]. How many tests are given and to who varies systematically in response to conditions on the ground with both periods of rationing and blitzes.

Measuring the effect of interventions is difficult because they are assigned endogenously in response to both local conditions and national signals. Similarly,  populations responded to both government orders and local conditions, often reducing their activity prior to being ordered to and also increasing their activity prior to being officially allowed to. Governments, the public, and the disease are all responding simultaneously to each other in often nonlinear and unobserved ways. Statistical instruments that cause government interventions but do not directly cause testing rates or rate of spread except through the government intervention are few and far between. Further, interventions are often implemented simultaneously or in a rolling cumulative pattern directly in response to changes in cases and testing results, making isolating the effect of any one treatment exceptionally difficult. 

Even if we had an exogenous intervention, its treatment effect on the rate of spread is still unlikely to be identified since almost any intervention will affect both cases and testing. Estimating an effect on just spread requires imposing additional assumptions, e.g. sharp constraints on some parameters and informative priors on the relationship between the number of tests and the number of cases [@kubinecRetrospectiveBayesianModel2020].

# Interpretation

One promising development is rigorous forecast evaluations [@nicholasgreichReichlabCovid19forecasthubPrepublication2020]. <!-- COVID-19 models serve both a short term forecasting role needed for making policy decisions and also a scientific role of identifying the actual underlying data generating process. In practice, many forecasting groups have focused almost entirely on the former, constantly tweaking their model to fit recent in-sample data without transparent theoretical justification.--> Notoriously, many early simple growth models fit to the takeoff period of infections performed well right up until the curve broke and then failed entirely. A parade of predicted peaks in cases since continue the tradition, with groups celebrating success on uninteresting short-term auto-correlations while ignoring failures on actually interesting shifts in trends. All we can do is develop a very long memory of predictions and constantly hold models accountable for their long run out-of-sample performance on unseen future data.

Other trends in COVID-19 work are less promising.^[We omit citations falling under the criticisms provided below as they are working papers and likely to change before finalization.] An overabundance of observational work still presents correlations as evidence of causation. Without identification, correlations on short highly autocorrelated time series are as likely to be misleading as informative. The SEIR model expects a nonlinear and highly auto-correlated pattern of an increasing infection rate that then levels off independent of any interventions. An unscrupulous, or naive, analyst can easily find interventions that increased (or decreased) spread solely by where those interventions land in the natural disease cycle, completely independent of the intervention's actual effect. 

Another bad habit is the pursuit of statistically distinguishable correlations over actually attempting to explain variation in COVID-19 outcomes themselves. Papers that can show a particular political party or demographic group is 'worse' on some COVID-19 dimension receive much attention. Such results lack strong explanatory power or clear policy recommendations, and so while great for making headlines, they do little to help us end the current pandemic.

Perhaps the most egregious trend in recent scholarship is setting up straw man null hypothesis and then presenting the inability to reject them as positive evidence for medical and safety decisions, e.g. social distancing might not be required because a model was unable to statistically distinguish a large uptick in cases following a mass-meeting. In the best of circumstances, absence of evidence is not evidence of absence. Our underfit, undertheorized, and underperforming observational models are not the best of circumstances, and they are not sufficiently sensitive to evaluate more than macro-level general trends.

# Conclusion

This necessarily brief review omitted positive developments in studying COVID-19 outside of macro-observational settings. There has been remarkable progress in areas of diagnosis, clinical treatment, and phylogenetic tracking. Data science has contributed to the rapid collaboration, development, and dissemination of research in a way not seen in prior disease outbreaks. We also neglected topics like tracing, and the accompanying contributions from the tech field such as monitoring through mobile apps and social media.   Further, our review is overly U.S.-centric, with other countries like South Korea monitoring the disease so effectively they succeeded at containment without having to resort to difficult mitigation.

Any research related to COVID-19 requires healthy caution of and respect for how little we actually know about the history of this pandemic. Practioners working on these questions and with these data will be deeply familiar with many of these concerns, but some may be especially subtle or less prominant  within one's main field of study. At a minimum, there is research being produced today which ignores much of these known methodological problems and subsequently generates confusion for novice consumers of analysis. We hope this enumeration of challenges in measurement, inference, and interpretation, can help both consumers and producers of COVID-19 knowledge alike.

<!-- Null results aren't good evidence

Contrarianism

Uncertainty is ok

Star hunting

# Conclusion





Depending on how closely one follows COVID-19 research, it may come as more or less of a surprise that our understanding of this pandemic is nowhere near that of the ideal. On the one hand, COVID-19 statistics are ubiquitous, over 300 new COVID-19 pre-publication papers are posted a day [@COVID19Primer], many countries have brought the spread under control, and there is race to complete trials on a number of vaccine candidates. On the other hand, there are a seemingly endless parade of reports on failures of testing, flip-flopping guidance on masks, flip-flopping guidance on mass-events, government efforts to limit or misrepresent case counts, etc. Not every country, has their spread under control, and many are just now beginning to enter their curves. These problems are just the tip of the iceberg for our issues of understanding this pandemic. Many of the things we take for granted are problematic in sometimes subtle ways.



Perhaps most taken for granted is that we have a clear understanding of how we got to this point, in terms of the introduction, spread, and decline of COVID-19 infections and deaths. We don't. Largely because, the institutional process of how measurements are taken, aggregated, and shared are often a larger source of variation than the actual empirical phenomenon. We have an increasingly sophisticated surveillance system, but its resolution is still too course to tell much more beyond general macro trends. Further, it started from such a low point, that most of the very important information about how the disease spread at the outset may be lost to time entirely.

Take the most widely reported figure, confirmed cases. As of this writing, the U.S. Centers for Disease Control reports 5.2 million cases in the United States [@cdcCoronavirusDisease20192020]. The CDC does not directly measure COVID-19 cases, it is an aggregator of reports from 60 local, state, territorial, and tribal public health authorities. These different jurisdictions do not always define cases the same way, nor are those definitions constant over time. ^[In April 14, 2020 the CDC started adding together both reports of confirmed and probable, even though some states don't report probable cases or aggregate the two.] The CDC has some ability to reach directly past states to hospitals and laboratories through the Emerging Infections Program (EIP) and the Influenza Hospitalization Surveillance Project (IHSP), but it is for a limited sample of participating locations. ^[The CARES Act in March of 2020 standardized some reporting requirements from states to the CDC, but then for unclear reasons reporting was abruptly transferred to Health and Human Services (HHS). The picture at the state level and below isn't better. States also are aggregaters from hospitals, healthcare providers, and laboratories.] Depending on their size, cities have their own additional reporting structure that rivals some states. 






# Problems of Inference

In practical terms, this means that one of the main inputs to our epidemiological models isn't fixed, it is the product of its own data generating process with both error and systematic determinants. Very few models take this into account, and at a minimum their confidence intervals should be considered a lower bound of uncertainty. That is especially true toward the early months of the pandemic when reporting was most quickly changing.


Surveys

One might want to try to use positivity rate instead of case counts, but as the case count rises, 

cases and deaths
IFR should be bimodal and varies over time.



Other efforts are focused on using novel signals 

COVID-19 as an instrument.

Model evaluation



Government responses "13,000 such policy announcements across more than 195 countries" [@chengCOVID19GovernmentResponse2020]

Medical work with bad statistics

Social science with sophisticated statistics but no epidemiological model

Star hunting rather than explaining

Little evaluation, short memories, constantly changing models and forcasts.


# Problems of Scientific Culture

Science is about being curious about the true state of the world. Intellectual curious is not just a motivation, but also an ethical guardrail against outside pressures. Too often, society rewards bad science with attention, prestige, and accolades. Bad science may be propped up in order to support preferred policies, justify profitable behavior, or simply as entertainment. COVID-19 is no exception, even though the stakes are literally life and death. To move beyond past failures, our work must be driven by intellectual curiousity. 

Contrarianism

Problems of contrarianism

Conformity
Similarly 

-Bad papers getting through
-Rough papers getting withdrawn
-General intolerance for uncertainty

The insistence we have an answer undermined the political and social capital we need to fund and find ethical designs for challenge studies that can actually answer the question. The shift in public campaign strategy generates distrust of experts and conspiracy theories making it more difficult to deliver expert guidance on other issues. There are always some predictable and unpredictable second order effects from misleading people. Over representing the evidence behind masks can mislead people into thinking that masks are more important than or trade off with more effective strategies like social distancing.

The dirty secret of COVID-19 modeling is that there just isn't that much unique information there. You can torture a panel dataset with a small number of units and a high auto-correlation between time units into telling you whatever you want, but that doesn't make it true.




# Conclusion

Don't be discouraged but do the reading.



Comparisons across these different collection highlight major discrepancies between the series, 
Most case counts are geographic, even though there is growing evidence of differential rates across age and race.



As political scientists, we are constantly running into data challenges as we try to understand concepts and behaviors that are hard to measure. In contrast, working with COVID-19 should be relatively straight forward. Governments are routinely reporting the number of cases and deaths which we can simply merge into our datasets and include in a regression. To do so would be to propagate a lie. Before we continue with any COVID research, we must show data humility and understand what we are actually working with.



Desperately looking for other kinds of measurements but no real ground truth
The sensors we use to estimate infections have their own complicated data generating processes which we also don't know and can't measure well.

-->


# References

<!-- <img src="https://hitcounter.pythonanywhere.com/count/tag.svg?url=https%3A%2F%2Frexdouglass.github.io%2FDataScienceAndCovid19%2FDataScienceAndCOVID19.nb.html" alt="Hits"> -->

