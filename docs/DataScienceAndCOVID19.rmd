---
title: "Data Science and COVID-19: 10 Current and Future Trends"
author:
  - name: Rex Douglass
    email: rexdouglass@gmail.com
    affiliation: University of California, San Diego
  - name: Thomas Leo Scherer
    email: tlscherer@ucsd.edu
    affiliation: University of California, San Diego
address:
  - code: University of California San Diego
    address: University of California, San Diego, La Jolla,CA, 92093
abstract: Abstract here

author_summary: |
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur eget porta erat. Morbi consectetur est vel gravida pretium. Suspendisse ut dui eu ante cursus gravida non sed sem. Nullam sapien tellus, commodo id velit id, eleifend volutpat quam. Phasellus mauris velit, dapibus finibus elementum vel, pulvinar non tellus. Nunc pellentesque pretium diam, quis maximus dolor faucibus id. Nunc convallis sodales ante, ut ullamcorper est egestas vitae. Nam sit amet enim ultrices, ultrices elit pulvinar, volutpat risus.

output: 
  html_notebook: default
  rticles::plos_article: default
bibliography: cpas_covid_19.bib

# Some paper adjustments
---

2000/2500 words

# Introduction

The SARS-COV-2 global pandemic has exposed weaknesses throughout our institutions and organizations, and the social sciences is no exception. Even today, a half a year into the deadliest crisis most of us will ever see, we still continue to fail to answer essential questions about the causes and consequences of the pandemic. We have assessed the record of COVID research and review some of the most prominent articles and debates. In this article, we detail the three virtues for all research to follow, COVID-related or not, so that we may avoide the same failures going forward. These virtues are data humility, intellectual curiousity, and disciplinary decency. 

[Can add 1-2 paragrafs summarizing body if need filler]

There is a growing effort to forecast how the world will differ after the COVID-19 pandemic. Unfortunately, such en effort presupposes a clarity of measurement and understanding about the COVID-19 pandemic that largely does not exist now and might never materialize. This brief note outlines ways in which the COVID-19 pandemic caught our political and scientific institutions off guard and how the global effort to catch up will be able to close the distance. Where possible we highlight some places for optimism, some places for improvement, and some places to level expectations.

To begin, we first describe our understanding of the pandemic in a hypothetical ideal world. In an ideal world, we would know to an approximation how the disease spreads in a population without any interventions, the rate of spread at time zero $R_0$, and we would know how that changes over time before, during, and after interventions, $R_t$. The testing regime that serves to measure this would be extensive, well understood, and uniform over time and space so that changes in known cases would be attributed to changes in the rate of spread and not the testing regime. Likewise, who dies from the disease would be well and consistently measured over time and space so that we both know the inherent danger of the disease, the Infected Fatality Rate (IFR), and we can see that rate go up or down in response to interventions and resource constraints. With the above in hand, we would be able to make careful, reasoned, an equitable decisions about how to make takeoffs in choices that have long term health benefits but short term economic costs or short term economic benefits but long term health costs. We could plan for when, not if, the next pandemic arrives, and successfully anticipate the long term changes to domestic and international affairs.

None of the above exists. Depending on how closely one follows COVID-19 research, this may come as more or less of a surprise. On the one hand, COVID-19 statistics are ubiquitous. The first research paper on COVID-19 was published January 21, 2020 and since then the research community has been averaging over 300 new pre-publication papers a day [@COVID19Primer]. Many countries have brought the spread under control, and there is race to complete trials on a number of vaccine candidates. On the other hand, there are a seemingly endless parade of reports on failures of testing, flip-flopping guidance on masks, flip-flopping guidance on mass-events, government efforts to limit or misrepresent case counts, etc. Not every country, has their spread under control, and many are just now beginning to enter their curves. Unfortunately, these problems are just the tip of the iceberg for our issues of understanding this pandemic. Many of the things we take for granted are also problematic in sometimes subtle ways.

Perhaps most taken for granted is that we have an understanding of how we got to this point, in terms of the introduction, spread, and decline of COVID-19 infections and deaths. The institutional data generating process of COVID-19 data is as important if not more important than the empirical data generating process we care about. How measurements are taken, aggregated, and shared are often a larger source of variation than the actual empirical phenomenon. This pandemic is no exception. Take the most widely reported figure, COVID-19 confirmed cases. As of this writing, the U.S. Centers for Disease Control reports 5.2 million cases in the United States [@cdcCoronavirusDisease20192020]. The CDC does not directly measure COVID-19 cases, it is an aggregator of reports from 60 local, state, territorial, and tribal public health authorities. As you might imagine, these different jurisdictions do not always define cases the same way, nor are those definitions constant over time. ^[In April 14, 2020 the CDC started adding together both reports of confirmed and probable, even though some states don't report probable cases or aggregate the two.] The CDC has some ability to reach directly past states to hospitals and laboratories through the Emerging Infections Program (EIP) and the Influenza Hospitalization Surveillance Project (IHSP), but it is for a limited sample of participating locations. The CARES Act in March of 2020 standardized some reporting requirements from states to the CDC, but then for unclear reasons reporting was abruptly transferred to Health and Human Services (HHS). The picture at the state level and below isn't better. States also are aggregaters from hospitals, healthcare providers, and laboratories. Depending on their size, cities have their own additional reporting structure that rivals some states. 

These bureaucracies are concerned primarily with releasing legally required contemporary measurements and not maintaining consistent historical time series. What has resulted is the world's largest most desperate data scavenger hunt to scrape, transcribe, and translate counts disseminated in oral briefings, public websites, pdfs, and even static images. Multiple teams from every country are working in often uncordinated and duplciated efforts to munge government reporting into consistent panel data covering places over time. The actors include newspapers and nonprofits like the New York Times [@albertsunNewYorkTimes2020] and USAfacts [@usafactsUSCoronavirusCases2020]. Private companies with existing largescale data collection efforts have stepped up including Bing [@chiqunzhangBingCOVID19Data2020] and Yahoo [@ashleywolfYahooKnowledgeGraph2020]. Consortiums of volunteers have emerged to write scrapers and collect counts by hand, 1Point3Acres [@yangCovidNetBringData2020], COVID Atlas li [@jzohrabCOVIDAtlasLi2020], Covid-19-India [@covid19indiaorg2020tracker]. Wikipedia has emerged as a near-real time massively crowdsourced authority.

The result is an ecosystem of fractured panel datasets, with varying holes in spatial and temporal coverage, and little metadata about sources, changes in definitions, and revisions to past counts from reporting sources. Comparisons between sources reveal worrying disagreements between aggregations [@wangComparingIntegratingUS2020]. They also reveal a number of idiosyncratic temporal issues, including delays in reporting, reporting luls around weekends and holidays, discontinuities and sudden revisions in counts upwards, or downwards, etc. It is not obvious how to correctly account for this problems or adjudicate between comflicting sources without a clear ground truth. There also isn't a archive of snapshots of the source material for many of these sources, meaning reconstructing these aggregatoins from raw sources may not be possible. 

In practical terms, this means that one of the main inputs to our empidimiological models isn't fixed, it is the product of its own data generating process with both error and systematic determinants. Very few models take this into account, and at a minimum their confidence intervals should be considered a lower bound of uncertainty. That is especially true toward the early months of the pandemic when reporting was most quickly changing. This is especially problematic because this is where we estimate R_0.

The elephant in the COVID-19 measurement room is the role of testing, and how it also has a seperate, poorly understood, poorly measured, data generating process that we tend to ignore. Efforts like the Covid-Tracking Project [@zachliptonCovidTrackingProject2020] have fought to collect state level testing data in the United States. All of the caveats for cases apply here, plus some additional ones about what kinds testing numbers (testing encounters, number of people tested ever, number of swabs tested, etc.). Further the type of test performed, and its false positive and false negative rate, is almost never included as metadata in aggregate test reports making comparison over time and location dubious.

The centrality of testing data generating process to the overall spread data generating process can't be understated. Per-capita testing rates are so strongly correlated with per-capita case rates, that cases are seen as nearly a proxy for testing, particularly in the early growth period of an outbreak [@kaashoekCOVID19PositiveCases2020a]. Spatially, per-capita testing rates also correlate with urbanity and co-morbids


Comparisons across these different collection highlight major discrepancies between the series, 


Most case counts are geographic, even though there is growing evidence of differential rates across age and race.


# Data Humility

As political scientists, we are constantly running into data challenges as we try to understand concepts and behaviors that are hard to measure. In contrast, working with COVID-19 should be relatively straight forward. Governments are routinely reporting the number of cases and deaths which we can simply merge into our datasets and include in a regression. To do so would be to propagate a lie. Before we continue with any COVID research, we must show data humility and understand what we are actually working with.

## 2)  Measurements are constantly being aggregated and reaggreated by different actors, and aren't being archived systematically.

There is no one in charge, there are no adults in the room. 

Lit review of sources [@alamoCovid19OpenDataResources2020]




cases and deaths
IFR should be bimodal and varies over time.

## 3) The measurements we have don't agree with one another,



## 4) Desperately looking for other kinds of measurements but no real ground truth

Surveys

Early warning [@koganEarlyWarningApproach2020]

## 5) The sensors we use to estimate infections have their own complicated data generating processes which we also don't know and can't measure well.



Handling testing induced effects correctly [@kubinecRetrospectiveBayesianModel2020]

## 1) We do not know how many people have had COVID-19 and we will likely never know.



# Intellectual Curiousity

Science is about being curious about the true state of the world. Intellectual curious is not just a motivation, but also an ethical guardrail against outside pressures. Too often, society rewards bad science with attention, prestige, and accolades. Bad science may be propped up in order to support preferred policies, justify profitable behavior, or simply as entertainment. COVID-19 is no exception, even though the stakes are literally life and death. To move beyond past failures, our work must be driven by intellectual curiousity. 


, and through application of evidence and methods, forming new more true beliefs than we held the day before.


## 6) Contrarianism complicates science

## 10) Star hunting rather than explaining

## 11) Little evaluation, short memories, constantly changing models and forcasts.



# Disciplinary Decency

[INTRO PARAGRAPH]

## 7) Conformity complicates science
-Bad papers getting through
-Rough papers getting withdrawn
-General intolerance for uncertainty

## 8) Medical work with bad statistics

## 9) Social science with sophisticated statistics but no epidemiological model












# More Scrap


Forecast Hub [@nicholasgreichReichlabCovid19forecasthubPrepublication2020]

Don't be discouraged but do the reading.
Bayesian estimate of R_t @[#BayesianNowcastingAdjustment]



Government responses "13,000 such policy announcements across more than 195 countries" [@chengCOVID19GovernmentResponse2020]



Dirty secrets - private institutions produce the data, government institutions aggregate it, private institutions aggregate it again.

We're almost always looking under the streetlamp.

We don't have mechanisms for sorting good evidence from bad.


New York Times partnered with Kaiser
Facts on File
CDC Flu View
serelogical evidene

Data science transforms information from one form into another, e.g. county PDFs into time series databases.

Data science studies the institutional data generating processes, reporting varies, that variation can and often is bigger than the signal empiricists want to test in their models.

Data science concerned with performance, and evaluation, e.g. forcasting and measuring performance.


The dirty secret of COVID-19 modeling is that there just isn't that much unique information there. You can torture a panel dataset with a small number of units and a high auto-correlation between time units into telling you whatever you want, but that doesn't make it true.


### Model evaluation and forcasting


Don't be discouraged but do the reading.

# References
